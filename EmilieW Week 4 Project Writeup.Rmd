---
title: "Week 4 Project Write Up "
Author: "Emilie Worsham"
Date: "12/28/2017"
output:
  html_document: default
  pdf_document: default
---
- Author: "Emilie Worsham"
- Date: "12/28/2017"
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Project Background
This project is the final project in the Practical Machine Learning Course. 

**Project Background and Objective from the Course Website:**
Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset).

## Data

The Following Data Sources were used for this project:

- Training Data: 
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

- Test Data:
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

- All of the data came from this source, and we thank them for allowing us to use this data for our projects:

 http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har. 

## Download and Clean the Training Data 

```{r download }

## download the training dataset 
download.file(url = "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", 
              destfile = "C:/Users/erobe/Desktop/Saved Items/Coursera/Practical_Machine_learning/Practical_Machine_Learning/pml-training.csv")

## Load training dataset
training <- read.csv("C:/Users/erobe/Desktop/Saved Items/Coursera/Practical_Machine_learning/Practical_Machine_Learning/pml-training.csv", na.strings=c("NA","#DIV/0!",""))

# Download the testing data
download.file(url = "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv", 
              destfile = "C:/Users/erobe/Desktop/Saved Items/Coursera/Practical_Machine_learning/Practical_Machine_Learning/pml-testing.csv")

# Load the testing dataset
testing <- read.csv("C:/Users/erobe/Desktop/Saved Items/Coursera/Practical_Machine_learning/Practical_Machine_Learning/pml-testing.csv", na.strings=c("NA","#DIV/0!",""))

```


## Exploring the Data & Cleaning the Data
First I wanted to see which columns are in both data sets. To be able to use both sets in modeling I will want to make sure that both sets have the same columns. 
```{r explore, results='asis'}
head(testing)
head(training)
```

As you can see most of the columns are in both sets. Now I would like to see how many null values are in each data set 
```{r nulls, }
## This function will find the null values in each column in testing 
na_count_testing <-sapply(testing, function(y) sum(length(which(is.na(y)))))

## Show how many nulls are in each column using the new function 
na_count_testing <- data.frame(na_count_testing)
na_count_testing
## This function will find the null values in each column in training 
na_count_training <-sapply(training, function(y) sum(length(which(is.na(y)))))

## Show how many nulls are in each column using the new function 
na_count_training <- data.frame(na_count_training)
na_count_training

```

As you can see there are quite a few nulls in both datasets so I will want to remove those before creating any of the models. 

```{r removenulls, }
nonull_training <- training[,colSums(is.na(training)) == 0] 
nonull_testing  <- testing[,colSums(is.na(testing)) == 0] 
head(nonull_training)
head(nonull_testing)
colnames(nonull_training, prefix =  'col')
colnames(nonull_testing, prefix =  'col')
```

Now the datasets have the same modeling columns, since we are using the model to predict the  "classe" variable the nonull_training dataset is the modeling dataset and the nonull_testing is now the crossvalidation dataset. This step is just to rename them for coding simplicity 

```{r rename, results= 'asis'}
testdf <- nonull_testing
traindf <- nonull_training
```


##Choosing Models
For this project we were given the ablity to choose which modeling techiniques we wanted to use. Since my job includes some modeling I will chose a few that we use most. 

- Decision Tree
- Random Forrest 



**Decision Tree Model**
```{r tree,  Warning = F, message=F}
library(rpart)
library(rpart.plot)

model_tree <- rpart(
                  classe ~ ., 
                  data=traindf,
                  method='class')

## view Model 
printcp(model_tree)

## Print Tree
plot(model_tree, uniform=TRUE, 
   	main="Decision Tree")
    text(model_tree, use.n=TRUE, all=TRUE, cex=.8)
    
    ## View Summary 
summary(model_tree)

## View RSquared

rsq.rpart(model_tree)
```
I'm not really happy with the outcome of the decision tree, so I'm hoping that the outcome of the Random Forest is better. 



## Using the Random Forest Model

Now I will set up the Random Forest Model, according to my coworkers, it is the model that they use 90% of the time so my guess before i get started is the one I will end up using to predict the outcome. 

```{r rfmodel, warning = F, }
library(randomForest)
library(caret)

## Split traindf into two groups. Model Development and Model Validation. 
sample.ind <- sample(2, 
                     nrow(traindf),
                     replace = T,
                     prob = c(0.6,0.4))
traindf_dev <- traindf[sample.ind==1,]
traindf_val <- traindf[sample.ind==2,]


## using the dev training data to create the model 
Model_Forest <- randomForest(classe ~ ., data=traindf_dev,  method='rf', ntree=2)

## Predicting using the model I just created using the training validation data set. 

pred_train_val <- predict(Model_Forest, traindf_val)



plot(Model_Forest)


## Looking at the importance of each variable in the model. 
varImpPlot(Model_Forest,
           sort = T,
           main="Variable Importance",
           n.var=20)

conf <- confusionMatrix(pred_train_val, traindf_val$classe)

conf


```


**As you can see this model has an accuracy of 97%! That's unheard of in the "real world" so clearly Random Forrest is our choice.**



### Using the Model to Predict the Outcome

Now I will use the Random Forest model I just built to predict the outcome using the testing data. 


```{r predictions, message=F, warning=F}
common <- intersect(names(traindf), names(testdf)) 
for (p in common) { 
  if (class(traindf[[p]]) == "factor") { 
    levels(testdf[[p]]) <- levels(traindf[[p]]) 
  } 
}

## Predictions for the course submission quiz. 
pred_val <- predict(Model_Forest, testdf[,names(testdf)!="problem_id"])
pred_val

```

After putting these values into the quiz you I can see that the random forest model and it's predictions was the one to use. 



